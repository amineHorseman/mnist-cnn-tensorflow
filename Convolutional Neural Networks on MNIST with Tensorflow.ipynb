{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A deep convolutional neural network to classify MNIST dataset using Tensorflow\n",
    "- To fully grasp the implementation of CNNs in tensorflow, we'll learn by classifying the simple MNIST dataset with 10 output classes. The basic idea of CNNs are the same even when we scale this up to the CIFAR-10 dataset.\n",
    "- This is an expert level technical guide on tensorflow based on \"Deep MNIST for Experts\" by Google \n",
    "https://www.tensorflow.org/get_started/mnist/pros , Keras is not used so we can fully understand the nuts and bolts of tensorflow and be able to customize and tweak it completely.\n",
    "- The theory of convolutional neural networks are not covered here as it is beyond the scope of this guide, please do some googling to understand kernels / filter sizes, strides, paddings and max pooling in CNNs before proceeding.\n",
    "\n",
    "by Kelvin Kong\n",
    "- **Github**: https://github.com/kelvinAI\n",
    "- **Linkedin**: https://linkedin.com/in/kelvinkong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Obtain the MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enable interactive output for easier debugging\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from IPython.core.debugger import set_trace\n",
    "#import necessary pacakages\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek into the mnist dataset data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000000008D78470>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000000008D78E10>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000000008D78BE0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets are stored in mnist.train, mnist.validation, mnist.test\n",
    "The datasets can be obtained in batches through the next_batch(batch_size) command\n",
    "\n",
    "First create placeholders for input nodes x and target output nodes y.\n",
    "\n",
    "The next_batch command returns a tensor with (batch_size, 28, 28 , 1) dimensions, where 28, 28 corresponds to the height and with of the image in pixels, and 1 for color channel (only greyscale).\n",
    "The flattened input will then be 784 ( 28 * 28 ) with 10 target output classes which corresponds to 0 - 9\n",
    "\n",
    "The target output classes y is a 2D tensor [ None , prediction] where prediction is a one-hot 10 dimensional vector that indicates which class that the image belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0]:(20, 784), batch[1]:(20, 10)\n",
      "batch[0]:(20, 784), batch[1]:(20, 10)\n",
      "batch[0]:(20, 784), batch[1]:(20, 10)\n",
      "batch[0]:(20, 784), batch[1]:(20, 10)\n",
      "batch[0]:(20, 784), batch[1]:(20, 10)\n"
     ]
    }
   ],
   "source": [
    "#Get an idea of the expected matrix shapes from the raw input to design our tensorflow placeholders\n",
    "batch_size = 20\n",
    "for i in range(100):\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    if i % 20 == 0:\n",
    "        print(\"batch[0]:{}, batch[1]:{}\".format(batch[0].shape, batch[1].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each batch\n",
    "batch[0] contains the training images in the dimensions [ batch_size, 784 ] where 784 is the 28 * 28 pixel image and batch_size corresponds to how many training examples that is computed in one single matrix multiplication step. Setting batch_size will directly impact the amount of RAM that is used, setting it too high may cause your machine to freeze. Typical values are around 128, 256 depending on your available RAM.\n",
    "\n",
    "batch[1] contains the one-hot encoded target output class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test x input shape: (20, 784)\n",
      "Test y input shape: (20, 10)\n",
      "Validate shape of x: [None, 784]\n",
      "Validate shape of y: [None, 10]\n"
     ]
    }
   ],
   "source": [
    "#First create placeholders to receive input values\n",
    "\n",
    "#Create some test input to validate the shape\n",
    "test_x_input = np.random.random((batch_size, 784))\n",
    "test_y_input = np.random.random((batch_size, 10))\n",
    "print(\"Test x input shape: {}\".format(test_x_input.shape))\n",
    "print(\"Test y input shape: {}\".format(test_y_input.shape))\n",
    "\n",
    "\n",
    "#Create a tensorflow placeholder of shape (batch_size, features) called x that will accept a matrix that have the shape of text_x_input, but use None\n",
    "#as the batch_size so we can change that later on the fly\n",
    "x = tf.placeholder(tf.float32, [None, test_x_input.shape[1]], name='x')\n",
    "print(\"Validate shape of x: {}\".format(x.get_shape().as_list()))\n",
    "\n",
    "#Same for y, create a placeholder to hold target output classes\n",
    "y = tf.placeholder(tf.float32, [None, test_y_input.shape[1]], name='y')\n",
    "print(\"Validate shape of y: {}\".format(y.get_shape().as_list()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks in Tensorflow\n",
    "Convolutional Neural Networks can be computed easily in tensorflow using tf.convo2d.\n",
    "Especially for first timers, we'll first understand the arguments needed by tf.convo2d to quickly clear the confusion of how to use this class properly.\n",
    "\n",
    "tf.convo2d( x_image_input_in_4d, weights, strides , padding)\n",
    "\n",
    "1. **x_image_input_in_4d** requires that your image input tensor to be in 4 dimensions, specifically ( batch_size, image height, image width, color depth)\n",
    "- for the MNIST dataset, the images are in greyscale, thus color depth == 1\n",
    "- MNIST dataset comes with an already flattened image that is 784 dimensions, thus we have to extract the height and image from these columns by reshaping it to 28*28. This can be done by using the tf.reshape command\n",
    "\n",
    "2. We will create a new placeholder **x_4d** instead to prevent confusion later on, by reshaping x. \n",
    "    \n",
    "    Note: -1 in the reshape command is a special key that means \"Automatically compute this column\". Only one -1 can be used at a time on any instance of reshaping.\n",
    "    Thus when we forcibly reshape the 1st, 2th and 3rd dimensions to 28, 28 and 1, we can put -1 as the 0th dimension and it will automatically be deduced from the matrix, which is in this case the batch_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_4d shape: [None, 28, 28, 1]\n"
     ]
    }
   ],
   "source": [
    "# x input needs to be reshaped to 4 dimensions for tf.convo2d.\n",
    "# [ batch_size, height, width, depth(Channels, 1 for grayscale)]\n",
    "x_4d = tf.reshape(x, [-1, 28, 28,1], name='x_4d')\n",
    "print(\"x_4d shape: {}\".format(x_4d.get_shape().as_list()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Convolutional Neural Network + Max Pooling Graph\n",
    "\n",
    "This is where the fun begins. The actual architecture of convolutional neural networks combined with max pooling, where the neural network will learn from the input image\n",
    "\n",
    "To facilitate understanding, (almost) the entire graph is created in the next cell. This is to help first timers in understanding the overall picture of the graph and not scrolling around blindly up and down across the notebook to search for global variables that are defined randomly in different cells. For larger networks, it is recommended to segregate some of the steps to reusable methods to improve readability and adhering to DRY.\n",
    "\n",
    "I find it easier to visualize the flow of the entire tensorflow graph this way before adding on sugar coated methods while refactoring the program. One thing to note, most of the tensorflow variables and placeholders here are defined in the global scope, but in the end you will only be using the last few of them in your tensorflow session as the graph have already been generated internally.\n",
    "\n",
    "This tutorial unfortunately does not explain the theory of convolutional neural networks in depth as it is beyond the scope, please do some googling to understand kernels / filter sizes, strides, paddings and max pooling in CNNs if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the first Weight/Filter: (5, 5, 1, 32)\n",
      "Bias dimensions of the first output layer will equal the number of output depth: (32,)\n",
      "Weight shapes for 2nd convolutional layer : [5, 5, 32, 64]\n",
      "Preview the maxpool_layer2 output dimensions: [None, 7, 7, 64]\n",
      "Reshaped FC layer Shape: (?, 3136)\n",
      "Weights of Fully connected layer 1: [3136, 1024]\n",
      "Fully connected layer 1 - Bias_fc1 shape :  [1024]\n",
      "fc_layer1_activation shape [None, 1024]\n",
      "Fully connected layer 1 output shape: [None, 1024]\n",
      "Weights shape for FC layer 2 : [1024, 10]\n",
      "Final FC layer2 shape, must be the same as the one hot encoded y input: [None, 10]\n",
      "\n",
      "Convolutional Neural Network Graph creation completed!\n"
     ]
    }
   ],
   "source": [
    "#Create first convolutional layer with depth of 32\n",
    "# ( x, kheight, kwidth, xdepth, output_depth)\n",
    "\n",
    "#weight is a 4 dimensional filter, (height, width, input_depth, output depth)\n",
    "output_depth_1 = 32\n",
    "\n",
    "#First, understand that weights for convolutional neural networks in tensorflow are of type tf.Variable and must\n",
    "#have 4 dimensions, (kernel height, kernel width, input_depth(color channels of input), output_depth)\n",
    "weight_1 = tf.Variable(tf.truncated_normal((5,5,1, output_depth_1),stddev=0.1) )\n",
    "print(\"Dimensions of the first Weight/Filter: {}\".format(weight_1.get_shape()))\n",
    "bias_1 = tf.Variable(tf.zeros(output_depth_1))\n",
    "print(\"Bias dimensions of the first output layer will equal the number of output depth: {}\".format(bias_1.get_shape()))\n",
    "\n",
    "#Create the first convolutional layer, using x_4d as input\n",
    "convo_layer1 = tf.nn.conv2d(x_4d, weight_1, strides=[1,1,1,1], padding='SAME' )\n",
    "convo_layer1 = tf.nn.bias_add(convo_layer1, bias_1)\n",
    "convo_layer1 = tf.nn.relu(convo_layer1) #Use relu activation function before applying max pooling\n",
    "\n",
    "#Apply max pooling on the convolutional layer 1 . note that the standard values for kernel size and strides for max pooling \n",
    "#are ksize=[1,2,2,1] and strides=[1,2,2,1]\n",
    "maxpool_layer1 = tf.nn.max_pool(convo_layer1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Obtain the depth of the maxpool_layer1 in the 3rd dimension\n",
    "maxpool_layer1_depth = maxpool_layer1.get_shape().as_list()[3]\n",
    "\n",
    "#Create the 2nd convolutional layer with depth of 64 for each 5x5 patch (determind by kernel size)\n",
    "convo_depth_2_output = 64\n",
    "\n",
    "weight_2 = tf.Variable(tf.truncated_normal((5,5,maxpool_layer1_depth, convo_depth_2_output)))\n",
    "print(\"Weight shapes for 2nd convolutional layer : {}\".format(weight_2.get_shape().as_list()))\n",
    "bias_2 = tf.Variable(tf.zeros(convo_depth_2_output))\n",
    "\n",
    "\n",
    "#Create the 2nd convolutional layer by applying convo2d on top of the previous maxpool_layer\n",
    "convo_layer2 = tf.nn.conv2d(maxpool_layer1, weight_2, strides=[1,1,1,1], padding='SAME') + bias_2\n",
    "convo_layer2 = tf.nn.relu(convo_layer2) \n",
    "\n",
    "#Apply the same maxpool layer\n",
    "maxpool_layer2 = tf.nn.max_pool(convo_layer2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "#Create the Dense / Fully Connected layer\n",
    "#First flatten the maxpool_layer2 with dimensions (None, 7, 7, 64)\n",
    "mx_layer2_dims = maxpool_layer2.get_shape().as_list()\n",
    "print(\"Preview the maxpool_layer2 output dimensions: {}\".format(mx_layer2_dims))\n",
    "\n",
    "# Now, to apply a fully connected later on this 4 dimension matrix, we'll first have to flatten back the matrix by\n",
    "# reshaping it to a 2d matrix which have the shape of 7 * 7 * 64 (based on the previous maxpool output layer)\n",
    "# We'll design it to automatically obtain the calculations by obtaining the shapes from the previous maxpool_layer2 and\n",
    "# multiply the 1st, 2nd and 3rd dimensions together. This is useful because if we change the kernel size, we do not \n",
    "# have to manually change this value\n",
    "\n",
    "fc_layer = tf.reshape(maxpool_layer2, [-1, mx_layer2_dims[1] * mx_layer2_dims[2] * mx_layer2_dims[3]])\n",
    "print(\"Reshaped FC layer Shape: {}\".format(fc_layer.get_shape()))\n",
    "\n",
    "fc_layer_dims = fc_layer.get_shape().as_list()\n",
    "\n",
    "# We'll create a first dense/hidden layer with 1024 output neurons, and a second dense layer with \n",
    "# 10 output neurons for final classification\n",
    "fc_layer_1_output = 1024\n",
    "\n",
    "weights_fc1 = tf.Variable(tf.truncated_normal([fc_layer_dims[1], 1024]))\n",
    "print(\"Weights of Fully connected layer 1: {}\".format(weights_fc1.get_shape().as_list()))\n",
    "bias_fc1 = tf.Variable(tf.zeros([fc_layer_1_output]))\n",
    "print(\"Fully connected layer 1 - Bias_fc1 shape : \", bias_fc1.get_shape().as_list())\n",
    "\n",
    "#Perform linear regression on the dense layer, add a bias, and apply RELU activation\n",
    "fc_layer1_z = tf.nn.bias_add(tf.matmul(fc_layer, weights_fc1),bias_fc1)\n",
    "fc_layer_1_relu = tf.nn.relu(fc_layer1_z)\n",
    "print(\"fc_layer1_activation shape\", fc_layer_1_relu.get_shape().as_list())\n",
    "\n",
    "#add Dropout on the first dense layer to prevent overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "fc_layer_1_drop = tf.nn.dropout(fc_layer_1_relu, keep_prob)\n",
    "\n",
    "#Final output layer\n",
    "dims_fc_l1_drop = fc_layer_1_drop.get_shape().as_list()\n",
    "print(\"Fully connected layer 1 output shape:\",dims_fc_l1_drop)\n",
    "\n",
    "# Create output with 10 classes\n",
    "# Simply a linear regression on the 1st fully connected layer, and add a bias. Note: no softmax activation functions here\n",
    "weights_fc2 = tf.Variable(tf.truncated_normal((dims_fc_l1_drop[1], 10)))\n",
    "print(\"Weights shape for FC layer 2 : {}\".format(weights_fc2.get_shape().as_list()))\n",
    "bias_fc2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "fc_layer2 = tf.nn.bias_add(tf.matmul(fc_layer_1_drop, weights_fc2),bias_fc2)\n",
    "print(\"Final FC layer2 shape, must be the same as the one hot encoded y input: {}\".format(fc_layer2.get_shape().as_list()))\n",
    "\n",
    "print(\"\\nConvolutional Neural Network Graph creation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll need to create the cost function and optimizer for the graph\n",
    "\n",
    "This can be done by using tf.nn.softmax_cross_entropy_with_logits where the labels argument is the target labels and logits will refer to your neural network's final output layer, which in this case is fc_layer2. The cross entropy is obtained by obtaining the mean value across this vector\n",
    "\n",
    "Instead of using gradient descent, we'll use the more sophisticated AdamOptimizer with a learning rate of 0.0004 and ask it to minimize the cross_entropy. Understand that cross_entropy in this case is not a single number, but rather a huge graph that was created from the above steps which includes fc_layer2, etc. By supplying cross_entropy into AdamOptimizer, tensorflow will automatically find a way to reduce the loss. (We'll have to treat it as a black box a this time when it comes to tensorflow as defining and writing your own gradient descent will not be feasible anymore when it comes to CNNs, but be sure to understand them)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create cross_entropy to be used in Adam optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=fc_layer2))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding how to calculate accuracy and predictions (Optional, for understanding purposes)\n",
    "Using this small tensorflow session, we'll understand how the accuracy tensor is created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_layer2 output shape:(5, 10)\n",
      "Peek into the 5 rows of final fully connected layer 2 output predictions:\n",
      "[[-1113.46716309  2792.99023438  2528.83447266  2069.26904297\n",
      "    103.74853516  1540.29199219  1262.55993652  2380.5222168  -1811.08862305\n",
      "    471.68908691]\n",
      " [  228.03097534  4267.12939453 -1583.92626953  -253.44909668\n",
      "   -228.01464844   798.46337891  6654.45458984  3446.76025391\n",
      "  -6469.01269531  -280.30090332]\n",
      " [ 1575.96398926  1041.82128906 -1055.50048828  1220.06860352\n",
      "    289.98596191   500.32275391  1477.18493652  3492.81640625\n",
      "  -1289.64929199  -410.78933716]\n",
      " [   78.37426758  3994.08032227  1848.76245117  2234.07055664\n",
      "   1012.12451172  1830.78198242  4707.67041016  3467.58081055\n",
      "  -3980.89819336  1479.26416016]\n",
      " [-1659.06079102  3980.03955078  1222.11865234   952.20593262\n",
      "  -2790.08544922  2176.875       1928.78955078  3106.7800293  -4728.12011719\n",
      "   1636.65942383]]\n",
      "tf argmax on predictions returns 5 values that are indexes of the largest value in each prediction row:\n",
      "[1 6 7 6 1]\n",
      "\n",
      "prediction[0] example:[-1113.46716309  2792.99023438  2528.83447266  2069.26904297   103.74853516\n",
      "  1540.29199219  1262.55993652  2380.5222168  -1811.08862305   471.68908691]\n",
      "column with highest prediction = pred_argmax[0] :1 \n",
      "\n",
      "Peek into 5 rows of target mnist_test_labels:\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "Index of columns with the correct target: [7 2 1 0 4]\n",
      "Predictions vs target:\n",
      "[1 6 7 6 1]\n",
      "[7 2 1 0 4]\n",
      "correct_predictions vector: [False False False False False]\n",
      "correct_predictions_in_numbers: [ 0.  0.  0.  0.  0.]\n",
      "Accuracy: 0.0\n",
      "\n",
      "Note:The accuracy at this stage should be terrible and totally random, since we have not trained the model yet.\n"
     ]
    }
   ],
   "source": [
    "# Warning, Calling this (particularly global_variables_initializer() )  will clear the weights if it had already been trained!\n",
    "# Example to obtain 5 predictions from the training dataset\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    predictions = sess.run(fc_layer2, feed_dict={x: mnist_test_x[:5], keep_prob: 1.0})\n",
    "    print(\"fc_layer2 output shape:{}\".format(predictions.shape)) #5 rows, 10 columns\n",
    "    \n",
    "    #Peek into 5 rows of predictions\n",
    "    print(\"Peek into the 5 rows of final fully connected layer 2 output predictions:\\n{}\".format(predictions))\n",
    "    \n",
    "    # Calling tf.argmax with axis = 1 on a predictions, will return the index/column that contains\n",
    "    # the largest value\n",
    "    \n",
    "    pred_argmax = sess.run(tf.argmax(predictions,1))\n",
    "    print(\"tf argmax on predictions returns 5 values that are indexes of the largest value in each prediction row:\\n{}\".format(pred_argmax))\n",
    "    print(\"\\nprediction[0] example:{}\\ncolumn with highest prediction = pred_argmax[0] :{} \".format(predictions[0], pred_argmax[0]))\n",
    "    \n",
    "    \n",
    "    print(\"\\nPeek into 5 rows of target mnist_test_labels:\\n{}\".format(mnist_test_labels[:5]))\n",
    "    actual_argmax = sess.run(tf.argmax(mnist_test_labels[:5], 1))\n",
    "    print(\"Index of columns with the correct target: {}\".format(actual_argmax))\n",
    "    \n",
    "    print(\"Predictions vs target:\\n{}\\n{}\".format(pred_argmax,actual_argmax))\n",
    "    \n",
    "    # We'll run tf.equals that will return a vector of TRUE FALSE values, if the highest prediction index in (pred_argmax)\n",
    "    # equals the index in target vector, then the prediction is correct. otherwise, if the network predicted a different \n",
    "    # class which resulted in a different index in pred_argmax, then the value will be false in that vector\n",
    "    correct_predictions = sess.run(tf.equal(pred_argmax,actual_argmax))\n",
    "    \n",
    "    print(\"correct_predictions vector: {}\".format(correct_predictions))\n",
    "\n",
    "    # We can obtain the accuracy of the predictions by dividing the total count of TRUE values over the entire prediction set\n",
    "    # First, cast the TRUE/False values to 1 or 0 using tf.cast to turn this TRUE/FALSE observation into a mathematical problem\n",
    "    prediction_numbers = sess.run(tf.cast(correct_predictions, tf.float32))\n",
    "    print(\"correct_predictions_in_numbers: {}\".format(prediction_numbers))\n",
    "\n",
    "    # Now calculate the accuracy by simply obtaining the mean of this vector, where 1 is a correct prediction and 0 is a wrong\n",
    "    # predcition\n",
    "    accuracy = sess.run(tf.reduce_mean(prediction_numbers))\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print(\"\\nNote:The accuracy at this stage should be terrible and totally random, since we have not trained the model yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating correct predictions and accuracy tensors\n",
    "Once we understand the above mechanics on predictions and accuracy, the two tensors is created in simply 2 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(fc_layer2,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll test our model on 1000 examples from the MNIST test database\n"
     ]
    }
   ],
   "source": [
    "#Create a subset of test images\n",
    "mnist_test_x = mnist.test.images[:1000]\n",
    "mnist_test_x.shape\n",
    "mnist_test_labels = mnist.test.labels[:1000]\n",
    "mnist_test_labels.shape\n",
    "\n",
    "print(\"We'll test our model on 1000 examples from the MNIST test database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:0.06599999964237213 0.0% completed\n",
      "Training accuracy:0.8830000162124634 10.0% completed\n",
      "Training accuracy:0.9229999780654907 20.0% completed\n",
      "Training accuracy:0.9359999895095825 30.0% completed\n",
      "Training accuracy:0.949999988079071 40.0% completed\n",
      "Training accuracy:0.9480000138282776 50.0% completed\n",
      "Training accuracy:0.9520000219345093 60.0% completed\n",
      "Training accuracy:0.9520000219345093 70.0% completed\n",
      "Training accuracy:0.9490000009536743 80.0% completed\n",
      "Training accuracy:0.9430000185966492 90.0% completed\n",
      "Train Complete!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "iterations = 10000\n",
    "reporting_count = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(iterations):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        output = sess.run(train_step, feed_dict={x:batch[0], keep_prob:0.5, y:batch[1]})\n",
    "        \n",
    "        if i % int(iterations/reporting_count) == 0:\n",
    "            print(\"Training accuracy:{} {}% completed\".format(\n",
    "                sess.run(accuracy,feed_dict={x:mnist_test_x, y:mnist_test_labels, keep_prob:1.0 }), i/iterations * 100 ))\n",
    "    print(\"Train Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Accuracy after training using a larger test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test images: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of test images: {}\".format(len(mnist.test.images)))\n",
    "# Create a test set of 5000 images from the whole set (only necessary to overcome RAM limitations, \n",
    "# otherwise use the full test set)\n",
    "mnist_test_x_large = mnist.test.images[:-1000]\n",
    "mnist_test_y_large = mnist.test.labels[:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy :0.08299999684095383\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(accuracy, feed_dict={x:mnist_test_x, y:mnist_test_labels, keep_prob:1.0 })\n",
    "    print(\"Final Accuracy :{}\".format(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
