{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A deep convolutional neural network to classify MNIST dataset using Tensorflow\n",
    "- To fully grasp the implementation of CNNs in tensorflow, we'll learn by classifying the simple MNIST dataset with 10 output classes. The basic idea of CNNs are the same even when we scale this up to the CIFAR-10 dataset.\n",
    "- This is an expert level technical guide on tensorflow based on \"Deep MNIST for Experts\" by Google \n",
    "https://www.tensorflow.org/get_started/mnist/pros , Keras is not used so we can fully understand the nuts and bolts of tensorflow and be able to customize and tweak it completely.\n",
    "- The theory of convolutional neural networks are not covered here as it is beyond the scope of this guide, please do some googling to understand kernels / filter sizes, strides, paddings and max pooling in CNNs before proceeding.\n",
    "\n",
    "by Kelvin Kong\n",
    "- **Github**: https://github.com/kelvinAI\n",
    "- **Linkedin**: https://linkedin.com/in/kelvinkong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Obtain the MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enable interactive output for easier debugging\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from IPython.core.debugger import set_trace\n",
    "#import necessary pacakages\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek into the mnist dataset data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000000008C98BA8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000000008D43EB8>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000000008D4E358>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets are stored in mnist.train, mnist.validation, mnist.test\n",
    "The datasets can be obtained in batches through the next_batch(batch_size) command\n",
    "\n",
    "First create placeholders for input nodes x and target output nodes y.\n",
    "\n",
    "The next_batch command returns a tensor with (batch_size, 28, 28 , 1) dimensions, where 28, 28 corresponds to the height and with of the image in pixels, and 1 for color channel (only greyscale).\n",
    "The flattened input will then be 784 ( 28 * 28 ) with 10 target output classes which corresponds to 0 - 9\n",
    "\n",
    "The target output classes y is a 2D tensor [ None , prediction] where prediction is a one-hot 10 dimensional vector that indicates which class that the image belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0]:(20, 784), batch[1]:(20, 10)\n"
     ]
    }
   ],
   "source": [
    "#Get an idea of the expected matrix shapes from the raw input to design our tensorflow placeholders\n",
    "batch_size = 20\n",
    "for i in range(1):\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    print(\"batch[0]:{}, batch[1]:{}\".format(batch[0].shape, batch[1].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each batch\n",
    "batch[0] contains the training images in the dimensions [ batch_size, 784 ] where 784 is the 28 * 28 pixel image and batch_size corresponds to how many training examples that is computed in one single matrix multiplication step. Setting batch_size will directly impact the amount of RAM that is used, setting it too high may cause your machine to freeze. Typical values are around 128, 256 depending on your available RAM.\n",
    "\n",
    "batch[1] contains the one-hot encoded target output class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test x input shape: (20, 784)\n",
      "Test y input shape: (20, 10)\n",
      "Validate shape of x: [None, 784]\n",
      "Validate shape of y: [None, 10]\n"
     ]
    }
   ],
   "source": [
    "#First create placeholders to receive input values\n",
    "\n",
    "#Create some test input to validate the shape\n",
    "test_x_input = np.random.random((batch_size, 784))\n",
    "test_y_input = np.random.random((batch_size, 10))\n",
    "print(\"Test x input shape: {}\".format(test_x_input.shape))\n",
    "print(\"Test y input shape: {}\".format(test_y_input.shape))\n",
    "\n",
    "\n",
    "#Create a tensorflow placeholder of shape (batch_size, features) called x that will accept a matrix that have the shape of text_x_input, but use None\n",
    "#as the batch_size so we can change that later on the fly\n",
    "x = tf.placeholder(tf.float32, [None, test_x_input.shape[1]], name='x')\n",
    "print(\"Validate shape of x: {}\".format(x.get_shape().as_list()))\n",
    "\n",
    "#Same for y, create a placeholder to hold target output classes\n",
    "y = tf.placeholder(tf.float32, [None, test_y_input.shape[1]], name='y')\n",
    "print(\"Validate shape of y: {}\".format(y.get_shape().as_list()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks in Tensorflow\n",
    "Convolutional Neural Networks can be computed easily in tensorflow using tf.convo2d.\n",
    "Especially for first timers, we'll first understand the arguments needed by tf.convo2d to quickly clear the confusion on how to use this class properly.\n",
    "\n",
    "tf.convo2d( x_image_input_in_4d, weights, strides , padding)\n",
    "\n",
    "1. **x_image_input_in_4d** requires that your image input tensor to be in 4 dimensions, specifically ( batch_size, image height, image width, color depth)\n",
    "- for the MNIST dataset, the images are in greyscale, thus color depth == 1\n",
    "- MNIST dataset comes with an already flattened image that is 784 dimensions, thus we have to extract the height and image from these columns by reshaping it to 28*28. This can be done by using the tf.reshape command\n",
    "\n",
    "2. We will create a new placeholder **x_4d** instead to prevent confusion later on, by reshaping x. \n",
    "    \n",
    "    Note: -1 in the reshape command is a special key that means \"Automatically compute this column\". Only one -1 can be used at a time on any instance of reshaping.\n",
    "    Thus when we forcibly reshape the 1st, 2th and 3rd dimensions to 28, 28 and 1, we can put -1 as the 0th dimension and it will automatically be deduced from the matrix, which is in this case the batch_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_4d shape: [None, 28, 28, 1]\n"
     ]
    }
   ],
   "source": [
    "# x input needs to be reshaped to 4 dimensions for tf.convo2d.\n",
    "# [ batch_size, height, width, depth(Channels, 1 for grayscale)]\n",
    "x_4d = tf.reshape(x, [-1, 28, 28,1], name='x_4d')\n",
    "print(\"x_4d shape: {}\".format(x_4d.get_shape().as_list()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Convolutional Neural Network + Max Pooling Graph\n",
    "\n",
    "This is where the fun begins. The actual architecture of convolutional neural networks combined with max pooling, where the neural network will learn from the input image\n",
    "\n",
    "To facilitate understanding, (almost) the entire graph is created in the next cell. This is to help first timers in understanding the overall picture of the graph and not scrolling around blindly up and down across the notebook to search for global variables that are defined randomly in different cells. For larger networks, it is recommended to segregate some of the steps to reusable methods to improve readability and adhering to DRY.\n",
    "\n",
    "I find it easier to visualize the flow of the entire tensorflow graph this way before adding on sugar coated methods while refactoring the program. One thing to note, most of the tensorflow variables and placeholders here are defined in the global scope, but in the end you will only be using the last few of them in your tensorflow session as the graph have already been generated internally.\n",
    "\n",
    "This tutorial unfortunately does not explain the theory of convolutional neural networks in depth as it is beyond the scope, please do some googling to understand kernels / filter sizes, strides, paddings and max pooling in CNNs if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the first Weight/Filter: (5, 5, 1, 32)\n",
      "Bias dimensions of the first output layer will equal the number of output depth: (32,)\n",
      "Weight shapes for 2nd convolutional layer : [5, 5, 32, 64]\n",
      "Preview the maxpool_layer2 output dimensions: [None, 7, 7, 64]\n",
      "Reshaped FC layer Shape: (?, 3136)\n",
      "Weights of Fully connected layer 1: [3136, 1024]\n",
      "Fully connected layer 1 - Bias_fc1 shape :  [1024]\n",
      "fc_layer1_activation shape [None, 1024]\n",
      "Fully connected layer 1 output shape: [None, 1024]\n",
      "Weights shape for FC layer 2 : [1024, 10]\n",
      "Final FC layer2 shape, must be the same as the one hot encoded y input: [None, 10]\n",
      "\n",
      "Convolutional Neural Network Graph creation completed!\n"
     ]
    }
   ],
   "source": [
    "#Create first convolutional layer with depth of 32\n",
    "# ( x, kheight, kwidth, xdepth, output_depth)\n",
    "\n",
    "#weight is a 4 dimensional filter, (height, width, input_depth, output depth)\n",
    "output_depth_1 = 32\n",
    "\n",
    "#First, understand that weights for convolutional neural networks in tensorflow are of type tf.Variable and must\n",
    "#have 4 dimensions, (kernel height, kernel width, input_depth(color channels of input), output_depth)\n",
    "\n",
    "# Create a default initial bias that is 0.1, that will be used \n",
    "# as a base for all biases in this network\n",
    "initia_bias = tf.constant(0.1)\n",
    "weight_1 = tf.Variable(tf.truncated_normal((5,5,1, output_depth_1),stddev=0.1) ,name='weights_1')\n",
    "print(\"Dimensions of the first Weight/Filter: {}\".format(weight_1.get_shape()))\n",
    "bias_1 = tf.Variable(tf.constant(0.1, shape=([output_depth_1])), name='bias_1')\n",
    "print(\"Bias dimensions of the first output layer will equal the number of output depth: {}\".format(bias_1.get_shape()))\n",
    "\n",
    "#Create the first convolutional layer, using x_4d as input\n",
    "convo_layer1 = tf.nn.conv2d(x_4d, weight_1, strides=[1,1,1,1], padding='SAME' )\n",
    "convo_layer1 = tf.nn.bias_add(convo_layer1, bias_1)\n",
    "convo_layer1 = tf.nn.relu(convo_layer1) #Use relu activation function before applying max pooling\n",
    "\n",
    "#Apply max pooling on the convolutional layer 1 . note that the standard values for kernel size and strides for max pooling \n",
    "#are ksize=[1,2,2,1] and strides=[1,2,2,1]\n",
    "maxpool_layer1 = tf.nn.max_pool(convo_layer1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Obtain the depth of the maxpool_layer1 in the 3rd dimension\n",
    "maxpool_layer1_depth = maxpool_layer1.get_shape().as_list()[3]\n",
    "\n",
    "#Create the 2nd convolutional layer with depth of 64 for each 5x5 patch (determind by kernel size)\n",
    "convo_depth_2_output = 64\n",
    "\n",
    "weight_2 = tf.Variable(tf.truncated_normal((5,5,maxpool_layer1_depth, convo_depth_2_output), stddev=0.1), name='weights_2')\n",
    "print(\"Weight shapes for 2nd convolutional layer : {}\".format(weight_2.get_shape().as_list()))\n",
    "bias_2 = tf.Variable(tf.constant(0.1,shape=[convo_depth_2_output]), name='bias_2')\n",
    "\n",
    "\n",
    "#Create the 2nd convolutional layer by applying convo2d on top of the previous maxpool_layer\n",
    "convo_layer2 = tf.nn.conv2d(maxpool_layer1, weight_2, strides=[1,1,1,1], padding='SAME') + bias_2\n",
    "convo_layer2 = tf.nn.relu(convo_layer2) \n",
    "\n",
    "#Apply the same maxpool layer\n",
    "maxpool_layer2 = tf.nn.max_pool(convo_layer2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "#Create the Dense / Fully Connected layer\n",
    "#First flatten the maxpool_layer2 with dimensions (None, 7, 7, 64)\n",
    "mx_layer2_dims = maxpool_layer2.get_shape().as_list()\n",
    "print(\"Preview the maxpool_layer2 output dimensions: {}\".format(mx_layer2_dims))\n",
    "\n",
    "# Now, to apply a fully connected later on this 4 dimension matrix, we'll first have to flatten back the matrix by\n",
    "# reshaping it to a 2d matrix which have the shape of 7 * 7 * 64 (based on the previous maxpool output layer)\n",
    "# We'll design it to automatically obtain the calculations by obtaining the shapes from the previous maxpool_layer2 and\n",
    "# multiply the 1st, 2nd and 3rd dimensions together. This is useful because if we change the kernel size, we do not \n",
    "# have to manually change this value\n",
    "\n",
    "fc_layer = tf.reshape(maxpool_layer2, [-1, mx_layer2_dims[1] * mx_layer2_dims[2] * mx_layer2_dims[3]])\n",
    "print(\"Reshaped FC layer Shape: {}\".format(fc_layer.get_shape()))\n",
    "\n",
    "fc_layer_dims = fc_layer.get_shape().as_list()\n",
    "\n",
    "# We'll create a first dense/hidden layer with 1024 output neurons, and a second dense layer with \n",
    "# 10 output neurons for final classification\n",
    "fc_layer_1_output = 1024\n",
    "\n",
    "weights_fc1 = tf.Variable(tf.truncated_normal([fc_layer_dims[1], 1024], stddev=0.1), name='weights_fc1')\n",
    "print(\"Weights of Fully connected layer 1: {}\".format(weights_fc1.get_shape().as_list()))\n",
    "bias_fc1 = tf.Variable(tf.constant(0.1, shape=[fc_layer_1_output]), name='bias_fc1')\n",
    "print(\"Fully connected layer 1 - Bias_fc1 shape : \", bias_fc1.get_shape().as_list())\n",
    "\n",
    "#Perform linear regression on the dense layer, add a bias, and apply RELU activation\n",
    "fc_layer1_z = tf.nn.bias_add(tf.matmul(fc_layer, weights_fc1),bias_fc1)\n",
    "fc_layer_1_relu = tf.nn.relu(fc_layer1_z)\n",
    "print(\"fc_layer1_activation shape\", fc_layer_1_relu.get_shape().as_list())\n",
    "\n",
    "#add Dropout on the first dense layer to prevent overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "fc_layer_1_drop = tf.nn.dropout(fc_layer_1_relu, keep_prob)\n",
    "\n",
    "#Final output layer\n",
    "dims_fc_l1_drop = fc_layer_1_drop.get_shape().as_list()\n",
    "print(\"Fully connected layer 1 output shape:\",dims_fc_l1_drop)\n",
    "\n",
    "# Create output with 10 classes\n",
    "# Simply a linear regression on the 1st fully connected layer, and add a bias. Note: no softmax activation functions here\n",
    "weights_fc2 = tf.Variable(tf.truncated_normal(shape=(dims_fc_l1_drop[1], 10), stddev=0.1), name='weights_fc2')\n",
    "print(\"Weights shape for FC layer 2 : {}\".format(weights_fc2.get_shape().as_list()))\n",
    "bias_fc2 = tf.Variable(tf.constant(0.1, shape=[10]),name='bias_fc2')\n",
    "\n",
    "fc_layer2 = tf.nn.bias_add(tf.matmul(fc_layer_1_drop, weights_fc2),bias_fc2)\n",
    "print(\"Final FC layer2 shape, must be the same as the one hot encoded y input: {}\".format(fc_layer2.get_shape().as_list()))\n",
    "\n",
    "print(\"\\nConvolutional Neural Network Graph creation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll need to create the cost function and optimizer for the graph\n",
    "\n",
    "This can be done by using tf.nn.softmax_cross_entropy_with_logits where the labels argument is the target labels and logits will refer to your neural network's final output layer, which in this case is fc_layer2. The cross entropy is obtained by obtaining the mean value across this vector\n",
    "\n",
    "Instead of using gradient descent, we'll use the more sophisticated AdamOptimizer with a learning rate of 0.0004 and ask it to minimize the cross_entropy. Understand that cross_entropy in this case is not a single number, but rather a huge graph that was created from the above steps which includes fc_layer2, etc. By supplying cross_entropy into AdamOptimizer, tensorflow will automatically find a way to reduce the loss. (We'll have to treat it as a black box a this time when it comes to tensorflow as defining and writing your own gradient descent will not be feasible anymore when it comes to CNNs, but be sure to understand them)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create cross_entropy to be used in Adam optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=fc_layer2))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding how to calculate accuracy and predictions (Optional, for understanding purposes)\n",
    "Using this small tensorflow session, we'll understand how the accuracy tensor is created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll test our model on 1000 examples from the MNIST test database\n"
     ]
    }
   ],
   "source": [
    "#Create a subset of test images\n",
    "mnist_test_x = mnist.test.images[:1000]\n",
    "mnist_test_x.shape\n",
    "mnist_test_labels = mnist.test.labels[:1000]\n",
    "mnist_test_labels.shape\n",
    "\n",
    "print(\"We'll test our model on 1000 examples from the MNIST test database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_layer2 output shape:(5, 10)\n",
      "Peek into the 5 rows of final fully connected layer 2 output predictions:\n",
      "[[  0.04432378  11.3144989   -1.62044585   3.14298773   3.07270193\n",
      "    1.24131954   3.44262147  -0.86096531  -1.12360168  -4.39922619]\n",
      " [ -5.96349764  10.39487076  -7.24298906   9.45944405   6.56074524\n",
      "    5.7898488    5.98724222   1.99842513   1.81409991  -0.88161957]\n",
      " [ -3.02583766   8.93795967  -3.44958329   5.1327076    1.59507525\n",
      "    1.11130035   1.81150401   0.06505308  -1.08163691  -0.66837418]\n",
      " [  2.80573869  11.72821522  -7.98079348   9.00608063   8.75436687\n",
      "    1.08036625  10.91279411  -3.48551512  -1.06569827  -2.80123901]\n",
      " [  2.3548274   13.22011948  -3.63398433   6.44128656   4.96160269\n",
      "    0.05033741   5.54397011  -0.03078761  -3.66377926  -2.96259117]]\n",
      "tf argmax on predictions returns 5 values (corresponding to 5 rows) that are indexes of the largest value in each predicted row:\n",
      "[1 1 1 1 1]\n",
      "\n",
      "Taking the 0th row (prediction[0]) as an example:[  0.04432378  11.3144989   -1.62044585   3.14298773   3.07270193\n",
      "   1.24131954   3.44262147  -0.86096531  -1.12360168  -4.39922619]\n",
      "The higheset value in this row is:13.22011947631836 which is column: [1]\n",
      "\n",
      "Peek into 5 rows of target mnist_test_labels:\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "Index of columns with the correct target: [7 2 1 0 4]\n",
      "Predictions vs target:\n",
      "[1 1 1 1 1]\n",
      "[7 2 1 0 4]\n",
      "correct_predictions vector: [False False  True False False]\n",
      "correct_predictions_in_numbers: [ 0.  0.  1.  0.  0.]\n",
      "Accuracy: 0.20000000298023224\n",
      "\n",
      "Note:The accuracy at this stage should be terrible and totally random, since we have not trained the model yet.\n"
     ]
    }
   ],
   "source": [
    "# Warning, Calling this (particularly global_variables_initializer() )  will clear the weights if it had already been trained!\n",
    "# Example to obtain 5 predictions from the training dataset\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    predictions = sess.run(fc_layer2, feed_dict={x: mnist_test_x[:5], keep_prob: 1.0})\n",
    "    print(\"fc_layer2 output shape:{}\".format(predictions.shape)) #5 rows, 10 columns\n",
    "    \n",
    "    #Peek into 5 rows of predictions\n",
    "    print(\"Peek into the 5 rows of final fully connected layer 2 output predictions:\\n{}\".format(predictions))\n",
    "    \n",
    "    # Calling tf.argmax with axis = 1 on a predictions, will return the index/column that contains\n",
    "    # the largest value\n",
    "    \n",
    "    pred_argmax = sess.run(tf.argmax(predictions,1))\n",
    "    print(\"tf argmax on predictions returns 5 values (corresponding to 5 rows) that are indexes of the largest value in each predicted row:\\n{}\".format(pred_argmax))\n",
    "    print(\"\\nTaking the 0th row (prediction[0]) as an example:{}\\nThe higheset value in this row is:{} which is column: [{}]\"\n",
    "          .format(predictions[0],np.max(predictions),pred_argmax[0]))\n",
    "    \n",
    "    \n",
    "    print(\"\\nPeek into 5 rows of target mnist_test_labels:\\n{}\".format(mnist_test_labels[:5]))\n",
    "    actual_argmax = sess.run(tf.argmax(mnist_test_labels[:5], 1))\n",
    "    print(\"Index of columns with the correct target: {}\".format(actual_argmax))\n",
    "    \n",
    "    print(\"Predictions vs target:\\n{}\\n{}\".format(pred_argmax,actual_argmax))\n",
    "    \n",
    "    # We'll run tf.equals that will return a vector of TRUE FALSE values, if the highest prediction index in (pred_argmax)\n",
    "    # equals the index in target vector, then the prediction is correct. otherwise, if the network predicted a different \n",
    "    # class which resulted in a different index in pred_argmax, then the value will be false in that vector\n",
    "    correct_predictions = sess.run(tf.equal(pred_argmax,actual_argmax))\n",
    "    \n",
    "    print(\"correct_predictions vector: {}\".format(correct_predictions))\n",
    "\n",
    "    # We can obtain the accuracy of the predictions by dividing the total count of TRUE values over the entire prediction set\n",
    "    # First, cast the TRUE/False values to 1 or 0 using tf.cast to turn this TRUE/FALSE observation into a mathematical problem\n",
    "    prediction_numbers = sess.run(tf.cast(correct_predictions, tf.float32))\n",
    "    print(\"correct_predictions_in_numbers: {}\".format(prediction_numbers))\n",
    "\n",
    "    # Now calculate the accuracy by simply obtaining the mean of this vector, where 1 is a correct prediction and 0 is a wrong\n",
    "    # prediction\n",
    "    accuracy = sess.run(tf.reduce_mean(prediction_numbers))\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print(\"\\nNote:The accuracy at this stage should be terrible and totally random, since we have not trained the model yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating correct predictions and accuracy tensors\n",
    "Once we understand the above mechanics on predictions and accuracy, the two tensors is created in simply 2 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(fc_layer2,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model and Remember to save it\n",
    "Training the model over the complete training dataset may take hours, so remember to save it!\n",
    "\n",
    "The variables will be automatically saved using the tf.Saver() class.\n",
    "It may be a good practice to explicitly name the variables too, such as the weights and bias using the name=\" \" argument while defining the weights/bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember to create a folder called saved_models in \n",
    "# the root dir before proceeding!\n",
    "# Note that models will be overwritten with the latest training, so take caution.\n",
    "# modify the script here to output in a new folder if you want to retain\n",
    "# some previously trained models\n",
    "save_file = './saved_models/cnn_model_0.ckpt' \n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:0.07999999821186066 0.0% completed\n",
      "Validation accuracy:0.9639999866485596 5.0% completed\n",
      "Validation accuracy:0.9779999852180481 10.0% completed\n",
      "Validation accuracy:0.9810000061988831 15.0% completed\n",
      "Validation accuracy:0.9850000143051147 20.0% completed\n",
      "Validation accuracy:0.9860000014305115 25.0% completed\n",
      "Validation accuracy:0.9860000014305115 30.0% completed\n",
      "Validation accuracy:0.9800000190734863 35.0% completed\n",
      "Validation accuracy:0.9850000143051147 40.0% completed\n",
      "Validation accuracy:0.9879999756813049 45.0% completed\n",
      "Validation accuracy:0.9879999756813049 50.0% completed\n",
      "Validation accuracy:0.9869999885559082 55.00000000000001% completed\n",
      "Validation accuracy:0.9909999966621399 60.0% completed\n",
      "Validation accuracy:0.9890000224113464 65.0% completed\n",
      "Validation accuracy:0.9879999756813049 70.0% completed\n",
      "Validation accuracy:0.9900000095367432 75.0% completed\n",
      "Validation accuracy:0.9909999966621399 80.0% completed\n",
      "Validation accuracy:0.9869999885559082 85.0% completed\n",
      "Validation accuracy:0.9900000095367432 90.0% completed\n",
      "Validation accuracy:0.9879999756813049 95.0% completed\n",
      "Train Complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./saved_models/cnn_model_22k.ckpt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as ./saved_models/cnn_model_22k.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Warning, running this cell may overwrite your previously saved model!\n",
    "batch_size = 50\n",
    "\n",
    "iterations = 20000\n",
    "reporting_count = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(iterations):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        output = sess.run(train_step, feed_dict={x:batch[0], keep_prob:0.5, y:batch[1]})\n",
    "#         train_step.run(feed_dict={x: batch[0], y:batch[1], keep_prob: 0.5})\n",
    "        if i % int(iterations/reporting_count) == 0:\n",
    "            print(\"Validation accuracy:{} {}% completed\".format(\n",
    "                sess.run(accuracy,feed_dict={x:mnist_test_x, y:mnist_test_labels, keep_prob:1.0 }), i/iterations * 100 ))\n",
    "#         print(\"Batch {} completed!\".format(i))\n",
    "    print(\"Train Complete!\")\n",
    "    \n",
    "    #Remember to save model!\n",
    "    saver.save(sess, save_file)\n",
    "    print(\"Model saved as {}\".format(save_file))\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Preview the saved models\n",
    "## Warning! there seems to be a bug here in the library, it doesn't close the \n",
    "## save_file after opening which leads to a permission error while\n",
    "## modifying the file. Remember to rename the save_file to\n",
    "## something elseif you intend to re-train again after running this section\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "print_tensors_in_checkpoint_file(save_file, 'weights_1', all_tensors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Accuracy after training using a larger test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test images: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of test images: {}\".format(len(mnist.test.images)))\n",
    "# Create a test set of 5000 images from the whole set (only necessary to overcome RAM limitations, \n",
    "# otherwise use the full test set)\n",
    "mnist_test_x_fromlast = mnist.test.images[-1000:]\n",
    "mnist_test_y_fromlast = mnist.test.labels[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the saved variables (weights and bias in particular) and use it to predict the test dataset\n",
    "\n",
    "Restore the variables by calling saver.restore(session, savefile)\n",
    "\n",
    "Note that if you call global_variable_initializer() here again, you are basically resetting your model with random weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy :99.40000176429749\n",
      "Accuracy Eval:99.40000176429749\n"
     ]
    }
   ],
   "source": [
    "best_trained_model_path = './saved_models/cnn_model_22k.ckpt' \n",
    "\n",
    "# Uncomment this to use your own trained model instead\n",
    "save_file = best_trained_model_path\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    output = sess.run(accuracy, feed_dict={x:mnist_test_x_fromlast, y:mnist_test_y_fromlast, keep_prob:1.0 })\n",
    "    print(\"Final Accuracy :{}\".format(output*100))\n",
    "    # Both ways are equivalent\n",
    "    print(\"Accuracy Eval:{}\".format(accuracy.eval(feed_dict={x:mnist_test_x_fromlast, y:mnist_test_y_fromlast, keep_prob:1.0 }) * 100 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "The initial bias parameter and tne initial standard deviation of the weights played a major role in affecting the final accuracy.\n",
    "In the earlier attempts, some of the weights were initialized randomly without supplying a standard deviation parameter, which led to being assigned the default value of 1.0. Also, the bias was set to all zeros, instead of 0.1. With these hyperparameters, it was observed that the maximum accuracy reached was around 94%. This demonstrates that the initial values of the weights and bias played a significant role in the model's accuracy, and must not be taken for granted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congrats! You've managed to build a deep convolutional model that's able to classify the MNIST test data with an accuracy of 99.4% ! With this knowledge, you can take on the CIFAR-10 dataset which contains pictures of trucks, airplanes, frogs, etc. The basic concept of image recognition using convolutions, max pooling, dropouts and fully connected (dense layers) are still the same. Customizing convolutions depths, filter shapes and strides to get the best prediction is more like an art where experience will play a major role.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
